{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c670fa-641f-4dec-8b35-09f20a5f94c8",
   "metadata": {},
   "source": [
    "### Importing the libraries to be used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf2e2a-3ecb-4336-a640-c609295ac43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19b70f9-4d04-49db-b53f-a1d30f7c4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from missingpy import MissForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97d71a-73c4-4e4e-a0e6-9a0562b31354",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311de5fd-096b-477d-8ec9-12083676b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = pd.read_csv('gender_submission.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2839d-639f-490e-bb3d-2b3daaa18b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Survived'].value_counts()/df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0914594-16a0-4982-a6a8-368b949369f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_summary_table(df):\n",
    "    \"\"\"\n",
    "    Returns a summary table showing null value counts and percentage\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataframe to check\n",
    "    \n",
    "    Returns:\n",
    "    null_values (DataFrame)\n",
    "    \"\"\"\n",
    "    null_values = pd.DataFrame(df.isnull().sum())\n",
    "    null_values[1] = null_values[0]/len(df)\n",
    "    null_values.columns = ['null_count','null_pct']\n",
    "    return null_values\n",
    "\n",
    "nulls_summary_table(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2bf19-8b44-4cee-913b-9ae5ea48407e",
   "metadata": {},
   "source": [
    "The below features hold very little predictive power, they hinder the models performance. Fare has very high correlation with Class, therefore I picked Class before it has a lower p-value than Fare against Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b75e0-9780-4bb7-8871-8fe647dbbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.drop(labels=['PassengerId','Cabin','Name','Ticket','SibSp','Fare','Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb5742-0286-406c-b5b2-5d725b6dc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b820e-19f8-45f9-9398-6b882b4accf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gender.drop(labels=['PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a911dd0-b41c-49ae-8e70-da016b502252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gender.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34806e24-6ce9-4246-9ae3-f33eb6ece2bb",
   "metadata": {},
   "source": [
    "The below features hold very little predictive power, they hinder the models performance. Fare has very high correlation with Class, therefore I picked Class before it has a lower p-value than Fare against Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec5275c-2b54-4072-a572-e5505d46f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(labels=['PassengerId','Cabin','Name','Ticket', 'Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b49ef31-64ec-4c53-b4c9-6f7ee9ce7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Sex'] = df_train['Sex'].map({'male':0, 'female':1}).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6839852-601a-43b0-b8a8-3c7588d5b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Embarked'] = df_train['Embarked'].map({'S':1,'C':2,'Q':3}).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e09406-fa5d-4c0d-802b-788ee98e8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b78bcae-d713-4001-87a1-78a680b9cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73f29db-5f8e-43bb-90f2-52d69a241f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28114526-89a6-4cb2-b537-ee62c187da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = ['Pclass','Sex','Parch','Embarked']\n",
    "for parameter in df_attributes:\n",
    "    pearson_coef, p_value = stats.pearsonr(df_train[parameter], df_train['Survived'])\n",
    "    print(parameter)\n",
    "    print('The Pearson Correlation Coefficient for ', parameter, ' is ', pearson_coef, 'with a P-value of P =', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a790ad44-0068-4a46-9259-d978f62e683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97fcc39-cff3-489d-a9fb-32b16b7f3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df.iloc[:,1:]\n",
    "targets = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4135716e-2a6c-410b-a1a1-beac9c11fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b97a5e-bdf5-40c5-a37b-c66c04c1a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaed8ed-f47b-4279-8f7f-c89b87e5de83",
   "metadata": {},
   "source": [
    "## Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6599c1b0-4b1b-42b7-8bc9-fa71708f3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets.shape[0]):\n",
    "    if targets[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \"to remove\" in the loop above.\n",
    "unscaled_inputs_equal_priors = np.delete(inputs, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c8b94e-ade4-453e-8d08-5c95596c7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = StandardScaler()\n",
    "scaled_inputs = transform.fit_transform(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3194209-6616-4abe-b558-36113fe34d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90589762, -0.84430228,  0.4382279 , -0.51751726, -0.53041081,\n",
       "        -0.59951774],\n",
       "       [-1.42306413,  1.18440992,  0.4382279 , -0.51751726,  0.63091985,\n",
       "         0.96845174],\n",
       "       [ 0.90589762,  1.18440992, -0.51080067, -0.51751726, -0.51816877,\n",
       "        -0.59951774],\n",
       "       ...,\n",
       "       [-0.25858325,  1.18440992, -0.51080067,  0.79558624, -0.19035415,\n",
       "        -0.59951774],\n",
       "       [-1.42306413,  1.18440992, -0.51080067, -0.51751726, -0.11780873,\n",
       "        -0.59951774],\n",
       "       [-1.42306413, -0.84430228, -0.51080067, -0.51751726, -0.11780873,\n",
       "         0.96845174]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4a04e-d00c-42dd-8e22-cb66a07904ff",
   "metadata": {},
   "source": [
    "### Shuffle The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40d07302-6c7e-4480-8808-b76a7462d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a4438-e07e-4099-a659-8058523cfafd",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2220d12b-dced-42ea-87a4-0715cedc7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276.0 544 0.5073529411764706\n",
      "29.0 68 0.4264705882352941\n",
      "35.0 68 0.5147058823529411\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743df14b-0343-4b9f-a48e-e94265fd70bf",
   "metadata": {},
   "source": [
    "## Save the datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2250320f-aed2-4e24-bfcb-a8adaf611f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Titanic_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Titanic_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Titanic_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
